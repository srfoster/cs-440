id: "m3-monte-carlo"
type: "short-answer"
chapter: 1
question: |
  **Play Connect-4 using Monte Carlo Tree Search when exact minimax becomes too slow.**

  While alpha-beta can search Connect-4 to depth 12-15, what if we want deeper search without exponential cost? Or what if we're playing a larger variant (8 columns × 7 rows)? Monte Carlo Tree Search (MCTS) provides an alternative: instead of exhaustively evaluating positions, we simulate many random games and trust that win rates reveal move quality. The key insight: even random play contains information. If a Connect-4 position wins 70% of random games, it's probably strong. This matters because MCTS scaled to Go (branching factor ~250), where alpha-beta failed.

  **Level 0** Discuss limitations of alpha-beta for Connect-4. At depth 15, we examine 7^15 ≈ 10^12 positions—possible but expensive. For deeper search or during opening (many options), we need alternatives. How does Monte Carlo simulation work? Why is "playing randomly informative"? Compare with 8-puzzle: there we had admissible heuristics (domain-specific knowledge); here we use random simulation (domain-independent).

  **Level 1** Describe simple Monte Carlo move evaluation for Connect-4. For each of 7 columns, drop a disc and simulate N=1000 random games (both players choose randomly until win/loss/draw). Track win rate per column. Choose column with highest win rate.
  
  Give pseudocode for MONTE_CARLO_MOVE(board, player, N) and RANDOM_PLAYOUT(board, player). Demonstrate by hand: Connect-4 position with 4 playable columns, run 8 random playouts per column (32 total), show how win rates differ.
  
  Then describe full MCTS with four phases applied to Connect-4: (1) **Selection** - traverse game tree using UCB formula to balance trying strong moves (exploitation) vs exploring uncertain moves (exploration), (2) **Expansion** - add new child node for unexplored column, (3) **Simulation** - random playout from new position, (4) **Backpropagation** - update win/visit counts up the tree. UCB1 formula: score = wins/visits + C√(ln(parent_visits)/visits). The second term encourages exploring moves we haven't tried much.

  **Level 2** Implement simple Monte Carlo for Connect-4: for each column, run 1000 random playouts, choose best. Then implement full MCTS: maintain tree of visited states, use UCB to traverse, expand one node per iteration, backpropagate results. Compare: (1) simple MC with 7000 playouts (1000 per column) vs (2) MCTS with 7000 iterations. Which plays better? Why does MCTS focus rollouts on promising moves?

  **Level 3** Analyze UCB1 trade-off in Connect-4. If a column has won 10/10 playouts, should we keep exploring it (exploitation) or try uncertain columns (exploration)? The √(ln(parent_visits)/visits) term decreases as visits increase, eventually forcing exploration. Prove UCB converges to optimal move. Discuss why MCTS works for Connect-4: branching factor 7 (manageable), games end quickly (30-42 moves), random play reaches terminal states. Compare with alpha-beta: when is each better? Connect-4 can be solved exactly with alpha-beta, but for games like Go (b≈250, 150+ moves), MCTS is necessary. Discuss AlphaGo: combining MCTS with neural networks to guide playouts.
answer: "Monte Carlo move selection evaluates each legal move by performing random playouts from the resulting state. For each move, run N simulations playing randomly until game end, record wins/losses, and choose the move with highest win rate. MCTS extends this with tree search, UCB selection, and incremental tree building."
topics:
  - "Monte Carlo Methods"
  - "MCTS"
  - "Stochastic Search"
vocab_answer:
  - word: "rollout"
    definition: "A simulated game played with random moves from a given state"
  - word: "UCB"
    definition: "Upper Confidence Bound - balances exploitation and exploration"
example_videos:
  - "https://www.youtube.com/watch?v=UXW2yZndl7U"
