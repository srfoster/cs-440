id: "m6-bayesian-networks"
type: "short-answer"
chapter: 3
question: |
  **Diagnose influenza using Bayesian networks with exact and approximate inference.**

  You're building a medical diagnosis system for flu. Patient presents with Fever and Cough. What's P(Flu|Fever, Cough)? The challenge: Flu, Cold, and Allergy have overlapping symptoms. Network structure: Season → Flu (flu more likely in winter), Flu → Fever, Flu → Cough, Flu → Fatigue, Cold → Fever, Cold → Cough, Allergy → Cough. Each variable binary (yes/no). Naive joint distribution: P(Season, Flu, Cold, Allergy, Fever, Cough, Fatigue) requires 2⁷=128 entries. With Bayesian network: only Σ 2^(parents+1) entries, exploiting conditional independence. Example: Fever depends only on Flu and Cold, not Season or Allergy. This matters because real medical systems have hundreds of diseases and symptoms—explicit joint distributions are impossible, but structured models work.

  **Level 0** Discuss why probabilistic reasoning differs from logic (Chapter 2). Logic: "If Flu then Fever" (deterministic). Probability: "P(Fever|Flu)=0.9" (uncertain). Why does medicine need uncertainty? Symptoms overlap, tests are imperfect, individual variation. Compare with CSP (Chapter 2): there constraints are hard (Sudoku cell = 5 or not), here probabilistic (Fever likely given Flu, but not certain). Why is inference hard? Exact inference NP-hard for general networks, but tractable for tree-structured networks (like CSP treewidth).

  **Level 1** Formulate flu diagnosis as Bayesian network. Variables: Season={Winter, Summer}, Flu, Cold, Allergy, Fever, Cough, Fatigue (all binary except Season). Edges: Season → Flu (P(Flu|Winter)=0.1, P(Flu|Summer)=0.01), Flu → Fever (P(Fever|Flu)=0.9), Cold → Fever (P(Fever|Cold)=0.7), combine with noisy-OR: P(Fever|Flu, Cold) = 1 - (1-0.9)(1-0.7) = 0.97. Similarly Flu → Cough (0.8), Cold → Cough (0.6), Allergy → Cough (0.9), combine with noisy-OR. Flu → Fatigue (0.95).
  
  **Exact Inference via Enumeration:** Query: P(Flu|Fever=yes, Cough=yes). Formula: P(Flu|e) = α P(Flu, e) = α Σ_{hidden} P(Flu, e, hidden). Hidden variables: Season, Cold, Allergy, Fatigue. Enumerate all 2⁴=16 combinations. Example: P(Flu=yes, Fever=yes, Cough=yes, Season=Winter, Cold=no, Allergy=no, Fatigue=yes) = P(Season=Winter) × P(Flu=yes|Winter) × P(Cold=no) × P(Allergy=no) × P(Fever=yes|Flu=yes,Cold=no) × P(Cough=yes|Flu=yes,Cold=no,Allergy=no) × P(Fatigue=yes|Flu=yes). Sum over all 16, normalize. Pseudocode: ENUM_ASK(X, e, bn): return NORMALIZE([ENUM_ALL(X=true, e), ENUM_ALL(X=false, e)]). ENUM_ALL(vars, e): if empty, return 1; else pick variable Y, if Y observed return P(Y=e[Y]|parents) × ENUM_ALL(rest, e); else return Σ_{y} P(Y=y|parents) × ENUM_ALL(rest, e).
  
  **Approximate Inference via Likelihood Weighting:** Generate N=1000 samples. For each sample: (1) Sample Season ~ P(Season) (e.g., Winter with probability 0.5). (2) Sample Flu ~ P(Flu|Season). (3) Fever is evidence (Fever=yes), so don't sample—fix Fever=yes and multiply weight w ×= P(Fever=yes|Flu,Cold). (4) Similarly fix Cough=yes, multiply w ×= P(Cough=yes|Flu,Cold,Allergy). (5) Sample other variables. After 1000 samples, estimate P(Flu=yes|e) ≈ (sum of w where Flu=yes) / (sum of all w). Pseudocode: LIKELIHOOD_WEIGHTING(X, e, bn, N): for i=1..N: (x,w) = WEIGHTED_SAMPLE(bn, e), if x[X]=true: W[true]+=w else W[false]+=w. Return NORMALIZE(W).

  **Level 2** Implement both methods for flu diagnosis network. Build network with 7 variables, CPTs as described (use noisy-OR for combining influences). Exact inference: ENUM_ASK for P(Flu|Fever=yes, Cough=yes). Should get ≈50-60% (depends on exact CPTs). Approximate: 1000 likelihood-weighted samples, compare with exact. Test sensitivity: P(Flu|Fever=yes, Cough=yes, Fatigue=yes) should be much higher (≈80-90%). Plot: Number of samples vs. error from true posterior.

  **Level 3** Prove joint distribution factorization: P(X₁,...,Xₙ) = ∏ P(Xᵢ|Parents(Xᵢ)) captures all independencies in DAG via d-separation. Analyze exact inference complexity: naive enumeration O(dⁿ) where d=domain size, n=variables. For flu: O(2⁷) = 128. Variable elimination: exploit network structure, eliminate variables in good order, complexity O(d^(w+1)) where w=induced width (related to treewidth). For tree-structured networks: O(nd²). Prove likelihood weighting converges: by law of large numbers, sample mean → true expectation as N→∞. Discuss why LW can be inefficient: if evidence unlikely (P(Fever=yes,Cough=yes|Flu=no) ≈50%), many samples get low weight, high variance. Compare with rejection sampling (wastes samples rejecting inconsistent ones), Gibbs sampling (MCMC, samples from joint), particle filtering (for temporal—next question). Real medical networks: 100s of nodes (symptoms, diseases, risk factors), hybrid discrete/continuous, learned from data (CPTs estimated from patient records).
answer: "Bayesian networks use DAG structure and CPTs to represent joint distributions compactly: P(X₁,...,Xₙ) = ∏P(Xᵢ|Parents(Xᵢ)). Exact inference (enumeration): sum over hidden variables, O(d^n) complexity. Variable elimination reduces to O(d^w) where w=treewidth. Approximate inference (likelihood weighting): generate weighted samples by fixing evidence and sampling non-evidence, weight by P(evidence|parents). Converges to true posterior as N→∞, but can be inefficient with unlikely evidence."
topics:
  - "Bayesian Networks"
  - "Probabilistic Inference"
  - "Likelihood Weighting"
  - "Variable Elimination"
vocab_answer:
  - word: "CPT"
    definition: "Conditional Probability Table defining P(X|Parents(X))"
  - word: "evidence"
    definition: "Observed variable values"
  - word: "likelihood weighting"
    definition: "Sampling method that fixes evidence and weights by its probability"
example_videos:
  - "https://www.youtube.com/watch?v=G-zirzQFWmk"
