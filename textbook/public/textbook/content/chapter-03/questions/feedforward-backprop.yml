id: "m7-feedforward-backprop"
type: "short-answer"
chapter: 3
question: |
  **Train a feedforward neural network to recognize handwritten digits (MNIST) using backpropagation.**

  MNIST: 60,000 training images of handwritten digits 0-9, each 28×28 pixels (784 dimensions). Task: classify digit from pixel intensities. Why neural networks? Hand-coding rules is infeasible ("if pixels at positions (12,15), (13,16), ... are dark, it's a 7")--too many patterns! Instead, learn a parametric function f(x; W, b) = softmax(W²(ReLU(W¹x + b¹)) + b²) with millions of parameters, adjusted to fit training data via gradient descent. Key innovation: backpropagation computes gradients efficiently in O(parameters) time via chain rule, enabling training of networks with billions of parameters. This matters because the same feedforward + backpropagation architecture scales from MNIST (simple) to ImageNet (1000 classes, millions of images) to GPT (trillions of parameters).

  **Level 0** Discuss universal approximation: a 2-layer network with enough hidden units can approximate any continuous function. Why does this matter? It means neural networks are theoretically expressive enough. But why were they impractical until 2000s? (1) Compute: training required GPUs (2012: AlexNet on ImageNet), (2) Data: needed large datasets (MNIST too small for deep nets, but ImageNet works), (3) Algorithms: better initialization (Xavier/He), activations (ReLU beats sigmoid), regularization (dropout, batch norm). Compare with previous AI: logic (exact symbolic rules), search (discrete actions), probability (hand-designed graphical models). Neural nets: differentiable, learned from data, approximate.

  **Level 1** **MNIST Neural Network:** Input x: 784-dimensional vector (flattened 28×28 image, pixel values 0-1). Hidden layer: 128 neurons. Output: 10 neurons (one per digit). Architecture:
  
  Layer 1: z¹ = W¹x + b¹ where W¹ is 128×784, b¹ is 128×1. h = ReLU(z¹) = max(0, z¹) (element-wise).
  
  Layer 2: z² = W²h + b² where W² is 10×128, b² is 10×1. p = softmax(z²) where pᵢ = exp(zᵢ²) / Σⱼ exp(zⱼ²). (Converts logits to probabilities summing to 1.)
  
  Loss: Cross-entropy L = -log(p[y]) where y is true digit (0-9). Why cross-entropy? Penalizes confident wrong predictions heavily.
  
  **Backpropagation:** Compute gradients via chain rule:
  
  ∂L/∂z²: For softmax + cross-entropy, ∂L/∂z² = p - one_hot(y). (Derivation: ∂(-log p_y)/∂z_i = p_i - δ_{iy}.) Call this δ².
  
  ∂L/∂W² = δ² hᵀ (outer product: 10×128). ∂L/∂b² = δ².
  
  ∂L/∂h = (W²)ᵀ δ² (backpropagate through W²).
  
  ∂L/∂z¹ = ∂L/∂h ⊙ ReLU'(z¹) where ReLU'(z) = 1 if z>0 else 0 (element-wise). Call this δ¹.
  
  ∂L/∂W¹ = δ¹ xᵀ (128×784). ∂L/∂b¹ = δ¹.
  
  Update: W¹ ← W¹ - α ∂L/∂W¹, etc. where α=learning rate (e.g., 0.01).
  
  Pseudocode:
  ```
  TRAIN_STEP_MNIST(x, y, W1, b1, W2, b2, lr):
    # Forward
    z1 = W1 @ x + b1
    h = max(0, z1)  # ReLU
    z2 = W2 @ h + b2
    p = softmax(z2)
    L = -log(p[y])
    
    # Backward
    d2 = p.copy(); d2[y] -= 1  # softmax+CE gradient
    dW2 = outer(d2, h)
    db2 = d2
    dh = W2.T @ d2
    d1 = dh * (z1 > 0)  # ReLU gradient
    dW1 = outer(d1, x)
    db1 = d1
    
    # Update
    W1 -= lr * dW1
    b1 -= lr * db1
    W2 -= lr * dW2
    b2 -= lr * db2
    return L
  ```
  
  Demonstrate by hand: 2 inputs (x=[0.5, 0.8]), 2 hidden neurons, 2 outputs (binary classification). Show forward pass computing z¹, h, z², p, L. Then backward pass computing gradients. One gradient descent step.

  **Level 2** Implement full MNIST classifier. Load MNIST (60K train, 10K test). Initialize W¹, b¹, W², b² with Xavier initialization: W ~ N(0, 2/(n_in + n_out)). Training loop: For 10 epochs, for each image (x,y), run TRAIN_STEP, average loss over epoch. Test: For each test image, predict = argmax(softmax(W² ReLU(W¹ x + b¹) + b²)), compute accuracy. Should get ≈97-98% with 128 hidden units. Plot: (1) Training loss vs. epoch (should decrease), (2) Test accuracy vs. epoch (should increase to ≈98%). Test with different hidden layer sizes: 32, 64, 128, 256. Larger = better accuracy but slower.

  **Level 3** **Universal Approximation Theorem (Cybenko 1989):** A 2-layer network with sigmoid activation and enough hidden units can approximate any continuous function on compact domain to arbitrary precision. Intuition: each hidden neuron learns a "bump" (sigmoid transition), combining them approximates any shape. Why depth helps: Deep networks learn hierarchical features (edges → textures → parts → objects) with exponentially fewer parameters than shallow networks. MNIST example: layer 1 learns edge detectors, layer 2 combines edges into digit shapes.
  
  **Vanishing/Exploding Gradients:** In deep networks, gradients ∝ (∂f/∂z)ᴸ where L=layers. If |∂f/∂z| < 1 (sigmoid), gradients vanish (→0). If > 1, explode (→∞). Solutions: (1) ReLU: ∂ReLU/∂z = 1 for z>0, gradients flow cleanly. (2) Residual connections (ResNet): f(x) = x + g(x), gradients += ∂g, always have path with gradient=1. (3) Batch normalization: normalize layer inputs, stabilizes gradients.
  
  **Initialization:** Random small weights (e.g., W ~ N(0, 0.01)) cause vanishing activations in deep nets. Xavier: W ~ N(0, √(2/(n_in+n_out))) keeps variance constant across layers. He initialization (for ReLU): W ~ N(0, √(2/n_in)).
  
  **Optimizers:** SGD: W ← W - α∇L. Momentum: v ← βv + ∇L, W ← W - αv (accelerates, smooths). Adam: adaptive learning rates per parameter, combines momentum + RMSProp. Most popular for training large models.
  
  **Backpropagation Complexity:** Forward: O(parameters) operations (matrix multiplies). Backward: also O(parameters) via reverse-mode automatic differentiation. This scalability enables billion-parameter models (GPT, next questions).
answer: "Forward pass: z1 = W1*x + b1, h = ReLU(z1), z2 = W2*h + b2, p = softmax(z2). Loss: L = -log(p[y]). Backward pass: dz2 = p (with dz2[y] -= 1 for cross-entropy gradient), propagate through W2 to get dh, apply ReLU derivative to get dz1, compute weight gradients dW1, dW2, db1, db2. Update: W -= lr * dW."
topics:
  - "Neural Networks"
  - "Backpropagation"
  - "Gradient Descent"
vocab_answer:
  - word: "activation function"
    definition: "Non-linear function applied to neuron outputs (e.g., ReLU, sigmoid)"
  - word: "gradient"
    definition: "Vector of partial derivatives indicating direction of steepest increase"
example_videos:
  - "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
