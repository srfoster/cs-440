id: "m7-feedforward-backprop"
type: "short-answer"
chapter: 3
question: |
  **Implement feedforward neural networks and backpropagation.**

  **Level 0** Discuss why neural networks are "universal approximators" yet were impractical until recent decades. What changed: compute (GPUs), data (internet-scale), and algorithms (better initialization, ReLU, normalization). Compare neural networks with previous AI approaches: logic (exact rules), search (discrete actions), probability (graphical models). Why are differentiable parametric models powerful?

  **Level 1** Define feedforward network: stack of layers f(x) = f_L(...f_2(f_1(x))). Each layer: z = Wx + b, h = activation(z). Common activations: ReLU(x) = max(0,x), sigmoid, tanh. Output layer: softmax for classification p_i = exp(z_i)/Σexp(z_j). Loss: cross-entropy L = -log(p_y).
  
  Describe backpropagation: compute ∂L/∂W via chain rule. For 2-layer MLP:
  - Forward: z1 = W1*x + b1, h = ReLU(z1), z2 = W2*h + b2, p = softmax(z2)
  - Loss: L = -log(p[y])
  - Backward: ∂L/∂z2 = p (with p[y] -= 1), ∂L/∂W2 = ∂L/∂z2 * h^T, ∂L/∂h = W2^T * ∂L/∂z2, ∂L/∂z1 = ∂L/∂h ⊙ ReLU'(z1), ∂L/∂W1 = ∂L/∂z1 * x^T
  - Update: W -= lr * ∂L/∂W
  
  Give pseudocode for one training step: TRAIN_STEP(x, y, params, lr). Demonstrate by hand with 2 inputs, 2 hidden units, 2 outputs.

  **Level 2** Implement 2-layer MLP for MNIST digit classification. Include forward pass, cross-entropy loss, backward pass computing all gradients, and parameter updates. Train for multiple epochs, track training loss and test accuracy.

  **Level 3** Prove universal approximation theorem (sketch): a 2-layer network with enough hidden units can approximate any continuous function. Discuss why depth helps: hierarchical features, parameter efficiency. Analyze gradient flow: vanishing/exploding gradients in deep networks, solutions (ReLU, residual connections, batch norm). Compare optimizers: SGD, momentum, Adam (adaptive learning rates). Discuss initialization: Xavier/He initialization prevents vanishing activations. Why does backpropagation scale: O(parameters) time via automatic differentiation.
answer: "Forward pass: z1 = W1*x + b1, h = ReLU(z1), z2 = W2*h + b2, p = softmax(z2). Loss: L = -log(p[y]). Backward pass: dz2 = p (with dz2[y] -= 1 for cross-entropy gradient), propagate through W2 to get dh, apply ReLU derivative to get dz1, compute weight gradients dW1, dW2, db1, db2. Update: W -= lr * dW."
topics:
  - "Neural Networks"
  - "Backpropagation"
  - "Gradient Descent"
vocab_answer:
  - word: "activation function"
    definition: "Non-linear function applied to neuron outputs (e.g., ReLU, sigmoid)"
  - word: "gradient"
    definition: "Vector of partial derivatives indicating direction of steepest increase"
example_videos:
  - "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
