id: "m7-language-model-training"
type: "short-answer"
chapter: 3
question: |
  **Implement the training loop for autoregressive language models.**

  **Level 0** Discuss the deceptive simplicity of LLM training: predict the next token. This single objective yields: grammar, facts, reasoning, coding ability. Why does next-token prediction work so well? It's a dense supervision signal (every token provides feedback), and language contains compressed knowledge about the world. Compare with: image classification (single label per image), reinforcement learning (sparse rewards). Historical context: statistical language models (n-grams), neural LMs (RNNs), transformer LMs (GPT, 2018), scaling laws (2020).

  **Level 1** Define autoregressive language modeling: P(text) = P(t₁) P(t₂|t₁) P(t₃|t₁,t₂) ... P(tₙ|t₁,...,tₙ₋₁). At training time, given sequence [t₁, t₂, ..., tₙ], predict tᵢ₊₁ from [t₁,...,tᵢ] for all i. Use causal masking so position i cannot see future tokens.
  
  **Training pipeline:**
  1. **Tokenization:** Convert text to integers via BPE/WordPiece tokenizer (vocab ~50K)
  2. **Batching:** Sample sequences of length L (e.g., 2048) from corpus
  3. **Forward:** Model outputs logits (T × vocab_size)
  4. **Loss:** Cross-entropy between logits[i] and target[i+1], averaged over all positions
  5. **Backward:** Compute gradients via backpropagation
  6. **Update:** Apply optimizer (AdamW with lr schedule, weight decay)
  
  Give pseudocode for TRAIN_LM(corpus, tokenizer, model, optimizer, steps, seqLen, batchSize). Demonstrate loss calculation on 5-token sequence.

  **Level 2** Implement LM training on small corpus (e.g., TinyShakespeare). Use 6-layer transformer with 512 model dim, 8 heads, context length 256. Track perplexity = exp(avg_loss). Implement teacher forcing: use ground truth tokens as context, not model predictions. Generate text via sampling: given prompt, predict next token, append, repeat. Compare temperature: low (confident, repetitive) vs high (diverse, incoherent).

  **Level 3** Analyze scaling laws: loss scales as power law in compute, data, parameters (L ∝ C^(-α)). GPT-3 (175B params) vs GPT-4 (rumored 1T+): bigger is better, but diminishing returns. Discuss training challenges: (1) memory - mixed precision (FP16), gradient checkpointing, model parallelism, (2) stability - gradient clipping, warm-up schedule, (3) data - curriculum learning, data filtering. Compare training objectives: next-token (GPT), masked (BERT), denoising (T5). Discuss instruction tuning (fine-tuning on QA pairs) and RLHF (reinforcement learning from human feedback). Why GPT architecture won: simplicity, parallelization, emergent capabilities at scale.
answer: |
  Training a language model involves predicting the next token given previous tokens. Sample sequences from the corpus, compute cross-entropy loss on next-token predictions, and update parameters via backpropagation.
  
  Pseudocode:
  ```
  function TRAIN_LM(corpusText, tokenizer, model, optimizer, steps, seqLen, batchSize):
      tokens = tokenizer.encode(corpusText)
      
      for step in 1..steps:
          batchX = []
          batchY = []
          for b in 1..batchSize:
              i = randomInt(0, len(tokens)-seqLen-2)
              x = tokens[i : i+seqLen]              # input tokens
              y = tokens[i+1 : i+seqLen+1]          # next-token targets
              batchX.append(x)
              batchY.append(y)
          
          logits = model.forward(batchX)            # (B, seqLen, vocab)
          L = CROSS_ENTROPY(logits, batchY)         # token-wise
          grads = model.backward(L)
          optimizer.step(model.params, grads)
      
      return model
  ```
topics:
  - "Language Models"
  - "Transformers"
  - "Next-Token Prediction"
example_videos:
  - "https://www.youtube.com/watch?v=kCc8FmEb1nY"
