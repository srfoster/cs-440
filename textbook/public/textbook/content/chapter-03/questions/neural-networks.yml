id: "m7-feedforward-backprop"
type: "short-answer"
chapter: 3
question: |
  **Implement forward and backward propagation on a neural network**

  **Level 0** What is a neural network and how are they significant?  How do forwards and backward propagation work?   

  **Level 1** Draw a simple neural network and a simple training dataset.  Demonstrate the forward pass and backpropagation.  Give the basic pseudocode and/or flowchart for training a machine learning model like a neural network.

  **Level 2**  Implement a simple feedforward neural network with one hidden layer and backpropagation in a language of your choice.  Use it to train on a small dataset (e.g., XOR problem).

  **Level 3**  Discuss the vanishing/exploding gradient problem in deep networks and how to mitigate it (e.g., ReLU, residual connections, batch normalization).  Discuss weight initialization strategies (Xavier, He) and optimizers (SGD, Momentum, Adam).  Analyze the computational complexity of backpropagation and how it scales with model size. 
answer: "Forward pass: z1 = W1*x + b1, h = ReLU(z1), z2 = W2*h + b2, p = softmax(z2). Loss: L = -log(p[y]). Backward pass: dz2 = p (with dz2[y] -= 1 for cross-entropy gradient), propagate through W2 to get dh, apply ReLU derivative to get dz1, compute weight gradients dW1, dW2, db1, db2. Update: W -= lr * dW."
topics:
  - "Neural Networks"
  - "Backpropagation"
  - "Gradient Descent"
vocab_answer:
  - word: "activation function"
    definition: "Non-linear function applied to neuron outputs (e.g., ReLU, sigmoid)"
  - word: "gradient"
    definition: "Vector of partial derivatives indicating direction of steepest increase"
example_videos:
  - "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
