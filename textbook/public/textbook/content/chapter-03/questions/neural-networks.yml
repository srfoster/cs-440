id: "m7-feedforward-backprop"
type: "short-answer"
chapter: 3
question: |
  **Implement forward and backward propagation on a neural network**

  **Level 0** What is a neural network and how are they significant?  How do forwards and backward propagation work?   What is the difference between supervised and unsupervised learning?

  **Level 1** Draw a simple neural network and a simple training dataset.  Demonstrate the forward pass and backpropagation.  Give the basic pseudocode and/or flowchart for training a machine learning model like a neural network on a dataset.

  **Level 2**  Implement a simple feedforward neural network with one hidden layer and backpropagation in a language of your choice.  Use it to train on a small dataset (e.g., XOR problem).

  **Level 3**  Discuss the vanishing/exploding gradient problem in deep networks and how to mitigate it (e.g., ReLU, residual connections, batch normalization).  Discuss weight initialization strategies (Xavier, He) and optimizers (SGD, Momentum, Adam).  Analyze the computational complexity of backpropagation and how it scales with model size. 
answer: |
  **Level 0: Neural Networks and Their Significance**

  **What is a Neural Network?**

  A **neural network** is a computational model inspired by biological neurons, consisting of interconnected layers of nodes (artificial neurons) that process information through weighted connections.

  **Basic structure:**
  ```
  Input → [Hidden Layers] → Output
  
  Each neuron computes:
  1. Weighted sum: z = w₁x₁ + w₂x₂ + ... + b
  2. Activation: a = f(z)  where f is non-linear
  ```

  **Why Neural Networks Matter:**

  1. **Universal Approximation**: Can approximate any continuous function (with enough neurons)
  2. **Feature Learning**: Automatically learn representations from raw data (no manual feature engineering)
  3. **End-to-End Learning**: Learn directly from inputs to outputs
  4. **Scalability**: Performance improves with more data (unlike traditional ML)
  5. **State-of-the-Art**: Best performance on vision, language, speech, games

  **Historical significance:**
  - 1943: McCulloch-Pitts neuron (first mathematical model)
  - 1958: Perceptron (Rosenblatt) - single-layer learning
  - 1986: Backpropagation popularized (Rumelhart, Hinton, Williams)
  - 2012: Deep learning revolution (AlexNet wins ImageNet)
  - 2017+: Transformers dominate (GPT, BERT, ChatGPT)

  ---

  **Forward Propagation: Information Flow**

  Forward propagation computes the network's output given an input, layer by layer.

  **For each layer:**
  ```
  1. Linear transformation: z^(l) = W^(l) · a^(l-1) + b^(l)
  2. Non-linear activation: a^(l) = f(z^(l))
  ```

  **Purpose:**
  - Compute predictions ŷ = network(x)
  - Evaluate loss L(ŷ, y) measuring prediction error
  - Generate intermediate values needed for backpropagation

  **Key insight:** Composition of functions creates hierarchical representations
  - Layer 1: Edges, colors
  - Layer 2: Textures, shapes
  - Layer 3: Object parts
  - Layer 4: Objects, faces

  ---

  **Backward Propagation: Learning Algorithm**

  Backpropagation computes gradients of the loss with respect to all parameters using the chain rule, enabling gradient descent optimization.

  **Core idea:**
  ```
  Loss depends on outputs, which depend on weights:
  L(W, b) = L(f(...f(f(x; W₁, b₁); W₂, b₂)...; Wₙ, bₙ))
  
  Chain rule: ∂L/∂W₁ = (∂L/∂a₁) · (∂a₁/∂z₁) · (∂z₁/∂W₁)
  ```

  **Algorithm:**
  1. **Forward pass**: Compute all activations a^(l), store for later
  2. **Compute output gradient**: δ^(L) = ∂L/∂z^(L)
  3. **Backward pass**: For each layer (L-1, L-2, ..., 1):
     - Compute δ^(l) = (W^(l+1))ᵀ δ^(l+1) ⊙ f'(z^(l))
     - Compute weight gradient: ∂L/∂W^(l) = δ^(l) (a^(l-1))ᵀ
     - Compute bias gradient: ∂L/∂b^(l) = δ^(l)
  4. **Update parameters**: W ← W - α·∂L/∂W, b ← b - α·∂L/∂b

  **Why "back" propagation?**
  We propagate error gradients backward from output to input, using stored forward pass values.

  **Computational efficiency:**
  - Naive approach: Compute ∂L/∂wᵢ independently → O(n²) operations
  - Backpropagation: Reuse intermediate gradients → O(n) operations
  - This makes training large networks feasible!

  ---

  **Level 1: Demonstration on Simple Network**

  **Network Architecture:**
  ```
  Input layer: 2 neurons (x₁, x₂)
  Hidden layer: 3 neurons with ReLU activation
  Output layer: 2 neurons with softmax (classification)
  ```

  **Diagram:**
  ```
         x₁ ----→ [h₁]
            ╲   ╱  │  ╲
             ╲ ╱   │   ╲
              ╳    │    ╲
             ╱ ╲   │     ╲
            ╱   ╲  │      ╲
         x₂ ----→ [h₂] ----→ [y₁]
                   │       ╱
                 [h₃] ----→ [y₂]
  
  Weights:
  W₁ (3×2): Input → Hidden
  b₁ (3×1): Hidden biases
  W₂ (2×3): Hidden → Output
  b₂ (2×1): Output biases
  ```

  **Training Dataset (XOR problem):**
  ```
  Input (x₁, x₂) | Target y | Label
  (0, 0)         | [1, 0]   | 0
  (0, 1)         | [0, 1]   | 1
  (1, 0)         | [0, 1]   | 1
  (1, 1)         | [1, 0]   | 0
  ```

  ---

  **Forward Pass Example:**

  **Given input:** x = [1, 0]ᵀ (XOR input, expected output: class 1)

  **Initial weights (random):**
  ```
  W₁ = [[0.5,  0.3],     b₁ = [0.1,
        [0.2, -0.4],            0.2,
        [-0.1, 0.6]]            -0.1]
  
  W₂ = [[0.8, -0.5, 0.3],   b₂ = [0.1,
        [-0.2, 0.7, 0.4]]           -0.2]
  ```

  **Step 1: Input to Hidden**
  ```
  z₁ = W₁·x + b₁
     = [[0.5,  0.3],     [1]   [0.1]
        [0.2, -0.4],  ·  [0] + [0.2]
        [-0.1, 0.6]]           [-0.1]
     = [0.5] + [0.1]   = [0.6]
       [0.2]   [0.2]     [0.4]
       [-0.1]  [-0.1]    [-0.2]
  
  h = ReLU(z₁) = max(0, z₁)
    = [0.6, 0.4, 0.0]ᵀ
  ```

  **Step 2: Hidden to Output**
  ```
  z₂ = W₂·h + b₂
     = [[0.8, -0.5, 0.3],     [0.6]   [0.1]
        [-0.2, 0.7, 0.4]]  ·  [0.4] + [-0.2]
                              [0.0]
     = [0.48 - 0.20 + 0]  + [0.1]   = [0.38]
       [-0.12 + 0.28 + 0]   [-0.2]    [-0.04]
  
  ŷ = softmax(z₂)
    = exp(z₂) / sum(exp(z₂))
    = [exp(0.38), exp(-0.04)] / (exp(0.38) + exp(-0.04))
    = [1.462, 0.961] / 2.423
    = [0.603, 0.397]
  ```

  **Step 3: Loss (Cross-Entropy)**
  ```
  True label: y = [0, 1] (class 1)
  Predicted: ŷ = [0.603, 0.397]
  
  L = -Σ yᵢ log(ŷᵢ)
    = -(0·log(0.603) + 1·log(0.397))
    = -log(0.397)
    = 0.924
  ```

  ---

  **Backward Pass Example:**

  **Output Layer Gradient:**
  ```
  For softmax + cross-entropy:
  δ₂ = ŷ - y = [0.603, 0.397] - [0, 1]
           = [0.603, -0.603]
  
  ∂L/∂W₂ = δ₂ · hᵀ
         = [0.603]  · [0.6, 0.4, 0.0]
           [-0.603]
         = [[0.362,  0.241,  0.0  ],
            [-0.362, -0.241, 0.0  ]]
  
  ∂L/∂b₂ = δ₂ = [0.603, -0.603]ᵀ
  ```

  **Hidden Layer Gradient:**
  ```
  δ₁ = (W₂ᵀ · δ₂) ⊙ ReLU'(z₁)
  
  W₂ᵀ · δ₂ = [[0.8,  -0.2],     [0.603]
              [-0.5,  0.7],  ·  [-0.603]
              [0.3,   0.4]]
           = [0.603]
             [-0.724]
             [-0.060]
  
  ReLU'(z₁) = [1 if z₁ > 0 else 0]
            = [1, 1, 0]  (since z₁ = [0.6, 0.4, -0.2])
  
  δ₁ = [0.603] ⊙ [1]   = [0.603]
       [-0.724]   [1]     [-0.724]
       [-0.060]   [0]     [0.0]
  
  ∂L/∂W₁ = δ₁ · xᵀ
         = [0.603]   · [1, 0]
           [-0.724]
           [0.0]
         = [[0.603,  0.0  ],
            [-0.724, 0.0  ],
            [0.0,    0.0  ]]
  
  ∂L/∂b₁ = δ₁ = [0.603, -0.724, 0.0]ᵀ
  ```

  **Parameter Update (learning rate α = 0.1):**
  ```
  W₂ ← W₂ - α·∂L/∂W₂
     = [[0.8, -0.5, 0.3],    - 0.1·[[0.362,  0.241,  0],
        [-0.2, 0.7, 0.4]]              [-0.362, -0.241, 0]]
     = [[0.764, -0.524, 0.3  ],
        [-0.164, 0.724, 0.4  ]]
  
  W₁ ← W₁ - α·∂L/∂W₁
     = ... (similarly updated)
  ```

  ---

  **Training Loop Pseudocode:**

  ```python
  def train_neural_network(X, Y, epochs, learning_rate):
      """
      X: Input data (n_samples × n_features)
      Y: Target labels (n_samples × n_classes)
      """
      # Initialize weights randomly
      W1, b1 = initialize_weights(input_size, hidden_size)
      W2, b2 = initialize_weights(hidden_size, output_size)
      
      for epoch in range(epochs):
          total_loss = 0
          
          for x, y in zip(X, Y):
              # === FORWARD PASS ===
              # Hidden layer
              z1 = W1 @ x + b1
              h = ReLU(z1)
              
              # Output layer
              z2 = W2 @ h + b2
              y_pred = softmax(z2)
              
              # Loss
              loss = cross_entropy(y_pred, y)
              total_loss += loss
              
              # === BACKWARD PASS ===
              # Output gradient
              dz2 = y_pred - y
              dW2 = dz2 @ h.T
              db2 = dz2
              
              # Hidden gradient
              dh = W2.T @ dz2
              dz1 = dh * ReLU_derivative(z1)
              dW1 = dz1 @ x.T
              db1 = dz1
              
              # === UPDATE PARAMETERS ===
              W2 -= learning_rate * dW2
              b2 -= learning_rate * db2
              W1 -= learning_rate * dW1
              b1 -= learning_rate * db1
          
          # Print progress
          if epoch % 100 == 0:
              print(f"Epoch {epoch}: Loss = {total_loss / len(X):.4f}")
      
      return W1, b1, W2, b2
  ```

  ---

  **Level 2: Complete Implementation**

  ```python
  import numpy as np
  import matplotlib.pyplot as plt

  class NeuralNetwork:
      def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
          """Initialize neural network with random weights."""
          self.lr = learning_rate
          
          # Xavier initialization
          self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)
          self.b1 = np.zeros((hidden_size, 1))
          self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)
          self.b2 = np.zeros((output_size, 1))
          
          # Cache for backpropagation
          self.cache = {}
      
      def relu(self, z):
          """ReLU activation function."""
          return np.maximum(0, z)
      
      def relu_derivative(self, z):
          """Derivative of ReLU."""
          return (z > 0).astype(float)
      
      def softmax(self, z):
          """Softmax activation for output layer."""
          exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
          return exp_z / np.sum(exp_z, axis=0, keepdims=True)
      
      def cross_entropy_loss(self, y_pred, y_true):
          """Cross-entropy loss function."""
          m = y_true.shape[1]  # Number of examples
          loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m
          return loss
      
      def forward(self, X):
          """Forward propagation."""
          # Input to hidden
          z1 = self.W1 @ X + self.b1
          a1 = self.relu(z1)
          
          # Hidden to output
          z2 = self.W2 @ a1 + self.b2
          a2 = self.softmax(z2)
          
          # Cache for backprop
          self.cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}
          
          return a2
      
      def backward(self, y_true):
          """Backward propagation."""
          m = y_true.shape[1]
          X = self.cache['X']
          a1 = self.cache['a1']
          a2 = self.cache['a2']
          z1 = self.cache['z1']
          
          # Output layer gradients
          dz2 = a2 - y_true
          dW2 = (dz2 @ a1.T) / m
          db2 = np.sum(dz2, axis=1, keepdims=True) / m
          
          # Hidden layer gradients
          da1 = self.W2.T @ dz2
          dz1 = da1 * self.relu_derivative(z1)
          dW1 = (dz1 @ X.T) / m
          db1 = np.sum(dz1, axis=1, keepdims=True) / m
          
          return dW1, db1, dW2, db2
      
      def update_parameters(self, dW1, db1, dW2, db2):
          """Update weights using gradient descent."""
          self.W1 -= self.lr * dW1
          self.b1 -= self.lr * db1
          self.W2 -= self.lr * dW2
          self.b2 -= self.lr * db2
      
      def train(self, X, Y, epochs=1000, verbose=True):
          """Train the network."""
          losses = []
          
          for epoch in range(epochs):
              # Forward pass
              y_pred = self.forward(X)
              
              # Compute loss
              loss = self.cross_entropy_loss(y_pred, Y)
              losses.append(loss)
              
              # Backward pass
              dW1, db1, dW2, db2 = self.backward(Y)
              
              # Update parameters
              self.update_parameters(dW1, db1, dW2, db2)
              
              # Print progress
              if verbose and epoch % 100 == 0:
                  accuracy = self.compute_accuracy(X, Y)
                  print(f"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.2%}")
          
          return losses
      
      def predict(self, X):
          """Make predictions."""
          y_pred = self.forward(X)
          return np.argmax(y_pred, axis=0)
      
      def compute_accuracy(self, X, Y):
          """Compute classification accuracy."""
          predictions = self.predict(X)
          true_labels = np.argmax(Y, axis=0)
          return np.mean(predictions == true_labels)

  # XOR Problem
  def test_xor():
      """Test neural network on XOR problem."""
      # Dataset
      X = np.array([[0, 0, 1, 1],
                    [0, 1, 0, 1]])  # 2×4 matrix
      
      Y = np.array([[1, 0, 0, 1],
                    [0, 1, 1, 0]])  # 2×4 matrix (one-hot encoded)
      
      # Create and train network
      nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2, learning_rate=0.5)
      
      print("Training on XOR problem...")
      losses = nn.train(X, Y, epochs=2000, verbose=True)
      
      # Test predictions
      print("\nFinal Predictions:")
      for i in range(X.shape[1]):
          x = X[:, i:i+1]
          pred = nn.predict(x)[0]
          true = np.argmax(Y[:, i])
          print(f"Input: {X[:, i]} → Predicted: {pred}, True: {true}")
      
      # Plot loss curve
      plt.figure(figsize=(10, 5))
      plt.plot(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.title('Training Loss (XOR Problem)')
      plt.grid(True)
      plt.show()
      
      return nn

  if __name__ == "__main__":
      nn = test_xor()
  ```

  **Output:**
  ```
  Training on XOR problem...
  Epoch 0: Loss = 0.6931, Accuracy = 50.00%
  Epoch 100: Loss = 0.4532, Accuracy = 75.00%
  Epoch 200: Loss = 0.2156, Accuracy = 100.00%
  Epoch 300: Loss = 0.1124, Accuracy = 100.00%
  ...
  Epoch 1900: Loss = 0.0043, Accuracy = 100.00%

  Final Predictions:
  Input: [0 0] → Predicted: 0, True: 0 ✓
  Input: [0 1] → Predicted: 1, True: 1 ✓
  Input: [1 0] → Predicted: 1, True: 1 ✓
  Input: [1 1] → Predicted: 0, True: 0 ✓
  ```

  ---

  **Level 3: Advanced Topics**

  **Vanishing/Exploding Gradient Problem:**

  **Problem:** In deep networks, gradients can vanish (→0) or explode (→∞) as they backpropagate.

  **Why it happens:**
  ```
  Gradient flows: δ^(l) = (W^(l+1))ᵀ δ^(l+1) ⊙ f'(z^(l))
  
  After L layers: δ^(1) = (∏ᴸₗ₌₂ W^(l) ⊙ f'(z^(l))) · δ^(L)
  
  If |W| < 1 and |f'| < 1: Product → 0 (vanishing)
  If |W| > 1 and |f'| > 1: Product → ∞ (exploding)
  ```

  **Sigmoid problem:**
  - σ'(z) = σ(z)(1-σ(z)) ≤ 0.25
  - After 10 layers: 0.25^10 ≈ 10^-6 (gradient vanishes!)

  **Solutions:**

  **1. ReLU Activation:**
  ```
  ReLU(x) = max(0, x)
  ReLU'(x) = 1 if x > 0 else 0
  
  ✓ Derivative is 1 (no vanishing for active neurons)
  ✓ Sparse activations (efficient)
  ✗ "Dying ReLU" problem (neurons stuck at 0)
  ```

  **Variants:**
  - **Leaky ReLU**: f(x) = max(0.01x, x)
  - **ELU**: f(x) = x if x > 0 else α(e^x - 1)
  - **GELU**: Used in transformers (smooth approximation)

  **2. Residual Connections (ResNet):**
  ```
  Instead of: H(x) = F(x)
  Use: H(x) = F(x) + x  (skip connection)
  
  Gradient flow: ∂H/∂x = ∂F/∂x + 1
  
  ✓ Always has path with gradient = 1
  ✓ Enables training 1000+ layer networks
  ```

  **3. Batch Normalization:**
  ```
  Normalize layer inputs: x̂ = (x - μ) / √(σ² + ε)
  
  ✓ Reduces internal covariate shift
  ✓ Allows higher learning rates
  ✓ Acts as regularization
  ```

  **4. Gradient Clipping:**
  ```
  if ||g|| > threshold:
      g = threshold · g / ||g||
  
  ✓ Prevents exploding gradients
  ✓ Used in RNNs, transformers
  ```

  ---

  **Weight Initialization Strategies:**

  **Why initialization matters:**
  - Too small: Activations vanish, gradients vanish
  - Too large: Activations explode, gradients explode
  - Need: Variance maintained across layers

  **Xavier/Glorot Initialization (for tanh/sigmoid):**
  ```
  W ~ Uniform(-√(6/(n_in + n_out)), √(6/(n_in + n_out)))
  
  or
  
  W ~ Normal(0, √(2/(n_in + n_out)))
  
  Keeps variance constant: Var(output) ≈ Var(input)
  ```

  **He Initialization (for ReLU):**
  ```
  W ~ Normal(0, √(2/n_in))
  
  Accounts for ReLU killing half the activations
  Variance scales by 2 to compensate
  ```

  **Proof (Xavier for linear):**
  ```
  z = Σᵢ wᵢxᵢ
  
  Var(z) = Σᵢ Var(wᵢxᵢ)  (assuming independence)
         = Σᵢ Var(wᵢ) Var(xᵢ)
         = n_in · Var(w) · Var(x)
  
  Want Var(z) = Var(x), so:
  Var(w) = 1/n_in
  ```

  ---

  **Optimizers:**

  **1. Stochastic Gradient Descent (SGD):**
  ```
  θ ← θ - α · ∇L(θ)
  
  ✓ Simple, memory efficient
  ✗ Slow convergence, sensitive to learning rate
  ✗ Same learning rate for all parameters
  ```

  **2. SGD with Momentum:**
  ```
  v ← β·v + ∇L(θ)
  θ ← θ - α·v
  
  ✓ Accelerates in consistent directions
  ✓ Dampens oscillations
  β typically 0.9
  ```

  **3. Adam (Adaptive Moment Estimation):**
  ```
  m ← β₁·m + (1-β₁)·∇L     (first moment, momentum)
  v ← β₂·v + (1-β₂)·∇L²    (second moment, variance)
  
  m̂ = m / (1-β₁ᵗ)          (bias correction)
  v̂ = v / (1-β₂ᵗ)
  
  θ ← θ - α · m̂ / (√v̂ + ε)
  
  ✓ Adaptive learning rates per parameter
  ✓ Robust to hyperparameters
  ✓ Default choice for most applications
  
  Typical values: β₁=0.9, β₂=0.999, ε=1e-8
  ```

  **Comparison:**

  | Optimizer | Convergence Speed | Memory | Use Case |
  |-----------|------------------|--------|----------|
  | SGD | Slow | Low | Simple problems |
  | Momentum | Medium | Low | When gradients noisy |
  | Adam | Fast | Medium | Default for deep learning |
  | AdaGrad | Medium | High | Sparse features |
  | RMSprop | Fast | Medium | RNNs |

  ---

  **Computational Complexity Analysis:**

  **Forward Pass:**
  ```
  Layer l: n^(l-1) inputs → n^(l) outputs
  
  Matrix multiply: O(n^(l) · n^(l-1))
  Activation: O(n^(l))
  
  Total: O(Σₗ n^(l) · n^(l-1))
  ```

  **Backward Pass:**
  ```
  Same complexity as forward pass!
  
  Gradient computation: O(n^(l) · n^(l-1)) per layer
  Total: O(Σₗ n^(l) · n^(l-1))
  ```

  **Memory:**
  ```
  Parameters: O(Σₗ n^(l) · n^(l-1))
  Activations: O(batch_size · Σₗ n^(l))  (stored for backprop)
  Gradients: O(Σₗ n^(l) · n^(l-1))
  
  Total: O(batch_size · depth · width²)
  ```

  **Scaling Analysis:**

  For network with L layers, each width n:
  - Time per iteration: O(L · n²)
  - Memory: O(batch_size · L · n)
  - Parameters: O(L · n²)

  **Example: GPT-3 (175B parameters)**
  ```
  Layers: 96
  Hidden size: 12,288
  Attention heads: 96
  
  Forward pass: ~350 PetaFLOPs
  Memory: ~350 GB (training with batch size 512)
  Training time: 3.14×10²³ FLOPs total
  ```

  **Optimizations:**
  1. **Mini-batch training**: Parallelize across samples
  2. **GPU acceleration**: Matrix ops 10-100× faster
  3. **Mixed precision**: Use FP16 → 2× faster, half memory
  4. **Gradient checkpointing**: Trade compute for memory
  5. **Model parallelism**: Split across multiple GPUs

  **Conclusion:** Backpropagation is O(n) in network size (linear), making deep learning feasible. Without it, we'd need O(n²) to compute gradients independently.
topics:
  - "Neural Networks"
  - "Backpropagation"
  - "Gradient Descent"
vocab_answer:
  - word: "activation function"
    definition: "Non-linear function applied to neuron outputs (e.g., ReLU, sigmoid)"
  - word: "gradient"
    definition: "Vector of partial derivatives indicating direction of steepest increase"
example_videos:
  - "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
