id: "m6-temporal-reasoning"
type: "short-answer"
chapter: 3
question: |
  **Track a disease outbreak over time using Hidden Markov Models and particle filtering.**

  An infectious disease spreads through a city. Day 0: 100 infected. Each day, infected individuals recover (20% rate) or infect others (each infects 0.3 new people on average). You observe hospital admissions (proxy for infections), but noisy: not all infected go to hospital (only 10%), and some admissions are other illnesses. Task: estimate true number of infected at day t given hospital data. This is temporal probabilistic reasoning: state Xᵗ (true infected count) evolves via dynamics, observations Eᵗ (hospital admissions) provide noisy evidence. Hidden Markov Model captures this: Xᵗ depends on Xᵗ₋₁ (transition model), Eᵗ depends on Xᵗ (observation model). Filtering computes P(Xᵗ|e₁:ᵗ). This matters for epidemic response: policymakers need real-time estimates of infection counts to decide interventions (lockdowns, vaccinations), but observations are delayed and noisy.

  **Level 0** Discuss why temporal reasoning differs from static Bayesian networks (previous question). There: one-shot diagnosis (Flu given symptoms). Here: tracking over time (infected count evolving). Markov assumption: Xᵗ depends only on Xᵗ₋₁ (not Xᵗ₋₂, Xᵗ₋₃, ...). Why is this both limiting and enabling? Limiting: real disease spread has memory (immunity, seasonal effects). Enabling: inference becomes O(T) instead of O(2ᵀ). Compare with search (Chapter 1): there we found paths, here we estimate hidden states. Filtering (current state), smoothing (past states), prediction (future states)—when is each needed?

  **Level 1** **SIR Disease Model as HMM:** State space Xᵗ = number infected (0 to N=1000). Transition model: P(Xᵗ₊₁ | Xᵗ). Simplified: new_infections ~ Poisson(0.3 × Xᵗ), recoveries ~ Binomial(Xᵗ, 0.2), so Xᵗ₊₁ = Xᵗ + new_infections - recoveries. Observation model: Eᵗ = hospital admissions ~ Poisson(0.1 × Xᵗ). Initial belief: P(X₀) = delta(100) (known start).
  
  **Exact Filtering (Forward Algorithm):** Maintain belief bᵗ = P(Xᵗ|e₁:ᵗ), a distribution over 0..1000. Update: (1) Predict: b̄ᵗ₊₁(x') = Σₓ P(x'|x) bᵗ(x). (2) Update: bᵗ₊₁(x) = α P(eᵗ₊₁|x) b̄ᵗ₊₁(x). Matrix form: bᵗ₊₁ = α Oᵗ₊₁ Tᵀ bᵗ where T = transition matrix, O = diagonal observation matrix. Pseudocode: HMM_FILTER(prior, observations, T, O): b = prior, for each e in observations: b = NORMALIZE(O[e] * (T.T @ b)). Demonstrate by hand: 3-state model (low/medium/high infection), 5 days of observations.
  
  **Approximate Filtering (Particle Filter):** When state space large (0..1000) or continuous, represent belief with N=1000 weighted particles. Each particle: hypothesis about Xᵗ (e.g., particle₁ = 150 infected). Algorithm: Day t→t+1: (1) Propagate: for each particle x, sample x' ~ P(Xᵗ₊₁|Xᵗ=x) (simulate disease dynamics with noise). (2) Weight: for each particle x', set w = P(eᵗ₊₁|x') (likelihood of observed hospital admissions). (3) Resample: draw N new particles with replacement, probability ∝ w. Pseudocode: PARTICLE_FILTER(particles, weights, e, TRANSITION, OBS_LIK): (1) propagate: particles' = [sample(TRANSITION(p)) for p in particles], (2) weights' = [OBS_LIK(e|p') for p' in particles'], (3) resample: particles = draw(particles', weights', N). Demonstrate: start with 1000 particles ≈ 100 infected, after observing admissions=15 (higher than expected for 100), particles shift toward ~150.

  **Level 2** Implement both for disease outbreak. **Exact HMM:** Discretize state space 0..1000 (or coarser: bins of 10). Build transition matrix T[x,x']: probability of x→x' (use Poisson/Binomial disease model, or approximate with Gaussian: x' ~ N(x + 0.3x - 0.2x, √(0.3x+0.2x))). Observation matrix O[e,x]: probability of observing e admissions given x infected (Poisson(0.1x)). Run forward algorithm for 30 days. Plot belief distribution over time. **Particle filter:** 1000 particles, propagate with stochastic disease dynamics, weight by Poisson likelihood of admissions. Compare: Does particle filter track exact belief? Test robustness: add outlier observation (e.g., admissions spike due to data error).

  **Level 3** Analyze complexity: Exact forward algorithm O(n²T) where n=states (1000), T=time (30), so 30M operations. Each step: predict matrix-vector multiply O(n²) if transition dense, O(n) if sparse (disease dynamics local: x→x' only for nearby x'). Update O(n). Particle filter O(NT) = 30K operations, much faster! But approximate. Prove convergence: as N→∞, particle distribution → true belief (by law of large numbers). Discuss particle degeneracy: after many steps, all weight on few particles. Solutions: (1) Effective sample size: resample when ESS < N/2, (2) Regularization: add noise during propagation, (3) Optimal proposal: sample from P(Xᵗ₊₁|Xᵗ, eᵗ₊₁) instead of just P(Xᵗ₊₁|Xᵗ). Compare with other temporal models: Kalman filter (assumes Gaussian beliefs, linear dynamics; works for disease if Gaussian approximation okay), EKF/UKF (nonlinear extensions), SLAM (simultaneous localization and mapping for robots: like tracking disease + estimating parameters), Viterbi (most likely sequence, not filtering distribution). Applications: epidemiology (COVID-19 tracking), wildlife population dynamics, financial time series.
answer: "HMM filtering maintains belief state P(Xt|e1:t) via predict-update: predict b̄t+1 = T^T bt, update bt+1 = α Ot+1 b̄t+1. O(n²) per step. Particle filtering approximates with N samples: propagate through transition, weight by P(e|x), resample proportional to weights. Handles continuous/large state spaces. O(N) per step but particle degeneracy can be problematic. Both are Bayesian updates combining prior dynamics with new observations."
topics:
  - "Hidden Markov Models"
  - "Particle Filtering"
  - "Temporal Reasoning"
  - "Bayesian Filtering"
vocab_answer:
  - word: "filtering"
    definition: "Estimating current state from past observations"
  - word: "belief state"
    definition: "Probability distribution over possible hidden states"
  - word: "particle"
    definition: "A sample representing one possible state hypothesis"
example_videos:
  - "https://www.youtube.com/watch?v=kqSzLo9fenk"
