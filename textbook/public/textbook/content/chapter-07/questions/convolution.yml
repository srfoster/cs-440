id: "m7-convolution"
type: "short-answer"
chapter: 7
question: |
  **Implement convolutional layers for computer vision.**

  **Level 0** Discuss why fully-connected layers are impractical for images: an image with 224×224×3 pixels has ~150K dimensions, requiring millions of parameters. How do convolutions solve this? Three key ideas: (1) local connectivity (each neuron only sees a small patch), (2) weight sharing (same filter applied everywhere), (3) translation invariance (detecting edges anywhere in the image). Compare with biological vision: receptive fields in the visual cortex.

  **Level 1** Define 2D convolution: slide a k×k filter (kernel) across an H×W image, computing dot products. For position (i,j), output = Σ_a Σ_b image[i+a][j+b] * kernel[a][b]. Output dimensions: (H-k+1) × (W-k+1) with stride=1, no padding.
  
  Extensions: **Stride** s (skip s pixels per step), **Padding** p (add p zeros around border), **Multiple channels** (RGB input, multiple filters). Output size: ⌊(H+2p-k)/s⌋ + 1.
  
  Give pseudocode for CONV2D(image, kernel, stride=1, padding=0). Demonstrate 3×3 filter detecting horizontal edges on a simple image.

  **Level 2** Implement CONV2D with stride and padding. Test filters: vertical edge detector [[1,0,-1],[1,0,-1],[1,0,-1]], horizontal edge detector (transpose), Gaussian blur. Implement a simple CNN: conv→ReLU→maxpool→conv→ReLU→maxpool→flatten→fully-connected. Train on MNIST or CIFAR-10.

  **Level 3** Analyze parameter efficiency: fully-connected layer for 224×224 images with 256 hidden units = 12M params. Convolutional layer with 256 filters of size 3×3 = 2K params (1000× reduction). Discuss receptive field growth through layers. Compare CNN architectures: LeNet (1998), AlexNet (2012), VGG (deeper stacks), ResNet (skip connections), modern vision transformers (ViT replaces convolution with attention). Why did CNNs dominate vision 2012-2020? Why are transformers now competitive?
answer: |
  Convolution is a fundamental operation in CNNs that applies a filter (kernel) across an image to detect features. The kernel slides over the image, computing dot products at each position.
  
  Pseudocode:
  ```
  function CONV2D(image, kernel):
      H = image.height; W = image.width
      kH = kernel.height; kW = kernel.width
      outH = H - kH + 1
      outW = W - kW + 1
      out = matrix(outH, outW, 0)
      
      for i in 0..outH-1:
          for j in 0..outW-1:
              s = 0
              for a in 0..kH-1:
                  for b in 0..kW-1:
                      s += image[i+a][j+b] * kernel[a][b]
              out[i][j] = s
      return out
  ```
topics:
  - "Convolution"
  - "CNNs"
  - "Computer Vision"
example_videos:
  - "https://www.youtube.com/watch?v=YRhxdVk_sIs"
