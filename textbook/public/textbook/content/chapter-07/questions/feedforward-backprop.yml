id: "m7-feedforward-backprop"
type: "short-answer"
chapter: 7
question: |
  A feedforward neural network composes linear transformations with nonlinear activations to learn complex functions. Backpropagation computes gradients of the loss with respect to parameters efficiently using the chain rule, enabling gradient-based training. This matters because it is the foundational mechanism behind deep learningâ€”understanding a single training step means you understand the computational core of training at any scale.
  
  **Task:** Write pseudocode for one training step of a 2-layer MLP with ReLU and softmax, computing loss, gradients, and parameter updates.
  
  **Level 2** Implement a training step for 2-layer MLP with ReLU activation and softmax output.
  
  **Level 3** Derive backpropagation equations from chain rule. Discuss gradient descent variants (SGD, Adam), initialization, and universal approximation theorem.
answer: "Forward pass: z1 = W1*x + b1, h = ReLU(z1), z2 = W2*h + b2, p = softmax(z2). Loss: L = -log(p[y]). Backward pass: dz2 = p (with dz2[y] -= 1 for cross-entropy gradient), propagate through W2 to get dh, apply ReLU derivative to get dz1, compute weight gradients dW1, dW2, db1, db2. Update: W -= lr * dW."
topics:
  - "Neural Networks"
  - "Backpropagation"
  - "Gradient Descent"
vocab_answer:
  - word: "activation function"
    definition: "Non-linear function applied to neuron outputs (e.g., ReLU, sigmoid)"
  - word: "gradient"
    definition: "Vector of partial derivatives indicating direction of steepest increase"
example_videos:
  - "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
