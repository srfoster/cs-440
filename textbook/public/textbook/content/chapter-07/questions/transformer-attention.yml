id: "m7-transformer-attention"
type: "short-answer"
chapter: 7
question: |
  Self-attention is the core operation of transformers. Given queries (Q), keys (K), and values (V), attention computes how much each token should "attend" to every other token, producing context-sensitive representations. The scaling factor 1/âˆšd stabilizes gradients when the key/query dimension is large. This matters because attention is the primary reason transformers outperform earlier sequence models on language tasks, and understanding it is essential for understanding LLMs.
  
  **Task:** Write pseudocode for ATTENTION(Q,K,V) using scaled dot-product attention and row-wise softmax.
  
  **Level 2** Implement single-head attention: scores = QK^T/sqrt(d), weights = softmax(scores), output = weights * V.
  
  **Level 3** Discuss multi-head attention, positional encoding, layer normalization, and training language models. Explain why transformers scale better than RNNs.
answer: "Self-attention computes attention(Q,K,V) = softmax(QK^T/sqrt(d)) * V, where Q,K,V are learned linear projections of input. Each position attends to all positions (unlike RNNs' sequential processing). Transformer blocks stack attention + feedforward with residual connections and layer norm. This parallel architecture enables efficient training on GPUs and captures long-range dependencies better than RNNs."
topics:
  - "Attention Mechanisms"
  - "Transformers"
  - "Self-Attention"
vocab_answer:
  - word: "query"
    definition: "Vector representing what information a position is looking for"
  - word: "key"
    definition: "Vector representing what information a position contains"
  - word: "value"
    definition: "Vector containing the actual information to be retrieved"
example_videos:
  - "https://www.youtube.com/watch?v=4Bdc55j80l8"
