id: "m7-transformer-block"
type: "short-answer"
chapter: 7
question: |
  A transformer block typically consists of (1) a self-attention sublayer and (2) a position-wise feedforward sublayer, each wrapped with residual connections and layer normalization. Residuals help optimization by preserving gradient flow; layer norm stabilizes activations. This matters because the transformer block is the basic repeating unit of LLMsâ€”understanding one block means you understand the architecture at scale.
  
  **Task:** Write pseudocode for TRANSFORMER_BLOCK(X, params) showing attention, residual, layer norm, feedforward, residual, and layer norm.
answer: |
  A transformer block combines multi-head self-attention with a feedforward network, using residual connections and layer normalization. This architecture enables powerful sequence modeling.
  
  Pseudocode:
  ```
  function TRANSFORMER_BLOCK(X, params):
      # X: (seqLen x dModel)
      A = ATTENTION(X*params.Wq, X*params.Wk, X*params.Wv) * params.Wo
      X1 = LAYERNORM(X + A)
      
      F = GELU(X1*params.W1 + params.b1) * params.W2 + params.b2
      Y = LAYERNORM(X1 + F)
      return Y
  ```
topics:
  - "Transformers"
  - "Deep Learning"
  - "Attention Mechanisms"
example_videos:
  - "https://www.youtube.com/watch?v=4Bdc55j80l8"
